{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd88ea27",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Exercise.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "# Exercise: Introduction to binary classification using logistic regression\n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "In this train, we'll build and refine a logistic regression model to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d230d14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to;\n",
    "- Understand logistic regression.\n",
    "- Implement a logistic regression model in sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1399649",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Breast Cancer Wisconsin (Diagnostic) dataset is a classic and very easy binary classification dataset. It contains features computed from a digitised image of a fine needle aspirate (FNA) of a breast mass, where each instance represents information related to a breast cancer cell nucleus. The goal is to classify whether the cell nuclei are **malignant** or **benign** based on these attributes.\n",
    "\n",
    "The dataset contains **30 numerical features**, such as mean radius, texture, perimeter, area, smoothness, compactness, and several others derived from the cell nuclei images. The target variable is binary: **malignant or benign**.\n",
    "\n",
    "First, let's load and examine our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3921ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "# list of features\n",
    "print(data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe9714",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Exploratory data analysis (EDA)\n",
    "Perform an exploratory data analysis on the Breast Cancer dataset. Focus on understanding the distribution of various features and the target variable. Create visualisations to understand the relationships between features and the target variable.\n",
    "\n",
    "1. To get a general idea of the characteristics of our data, plot the distribution of some of the features. Would we need to do this for all of the features in a real-world situation?\n",
    "   \n",
    "   **NOTE**: Use `mean radius` and `mean texture`.\n",
    "2. Create a correlation heatmap to identify the relationships between features.\n",
    "3. Analyse the balance between malignant and benign samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d367dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b6a66",
   "metadata": {},
   "source": [
    "### Exercise 2: Data preparation and model training\n",
    "Prepare the dataset for logistic regression. \n",
    "\n",
    "1. Split the data into training and test sets.\n",
    "2. Normalise the features using `StandardScaler` from `sklearn.preprocessing`.\n",
    "3. Train a logistic regression model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795081f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d86be26f",
   "metadata": {},
   "source": [
    "### Exercise 3: Model evaluation and interpretation\n",
    "Evaluate the performance of your logistic regression model and interpret the results.\n",
    "\n",
    "1. Calculate and interpret the model's accuracy on the test set.\n",
    "2. Use a confusion matrix and classification report to evaluate your model's performance.\n",
    "3. Interpret the model coefficients: Identify and discuss the top three features that contribute most to predicting malignancy in cell nuclei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff158fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5892f2f",
   "metadata": {},
   "source": [
    "### Challenge exercise: Model improvement\n",
    "Attempt to improve our model's performance through at least one of the following techniques:\n",
    "- **Feature selection**: Select a subset of relevant features to train your model.\n",
    "- **Hyperparameter tuning**: Experiment with different values for the hyperparameters of the logistic regression model (e.g. `C`, the regularisation strength)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555dea0d",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8f391",
   "metadata": {},
   "source": [
    "### Exercise 1: Exploratory data analysis (EDA)\n",
    "\n",
    "This exercise is designed to visualise and understand the characteristics of the Breast Cancer dataset from `sklearn.datasets`. By looking at the outputs, we can understand the distribution of individual features, how features relate to each other, and the balance of classes in the target variable. This information is crucial for deciding on the next steps in the data analysis or model-building process, such as feature selection, feature engineering, and choosing appropriate algorithms for classification or regression tasks.\n",
    "\n",
    "1. **Data import and DataFrame conversion:**\n",
    "   \n",
    "   This part of the code imports necessary libraries and converts the data into a `DataFrame` object, which is a 2D labelled data structure with columns of potentially different types. It's being used here to store features in `df_features` and the target variable in `df_target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df_features = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df_target = pd.DataFrame(data.target, columns=['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbdb88a",
   "metadata": {},
   "source": [
    "2. **Distribution plots:**\n",
    "\n",
    "   Here, distribution plots for the 'mean radius' and 'mean texture' are created using Seaborn's `displot` function, with 20 bins specified for each histogram. These plots help to visualise the distribution of these two features. The histograms are overlaid with kernel density estimates to show the distribution shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717f8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean radius distribution plot\n",
    "sns.displot(df_features['mean radius'], bins=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0973b",
   "metadata": {},
   "source": [
    "From the results, we can see that the 'mean radius' has a roughly normal distribution skewed to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb221757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean texture distribution plot\n",
    "sns.displot(df_features['mean texture'], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "243f1da9",
   "metadata": {},
   "source": [
    "`mean texture`, on the other hand, is more normally distributed.\n",
    "\n",
    "3. **Correlation heatmap:**\n",
    "  \n",
    "   The code below generates a heatmap of the correlation matrix of the features in `df_features`. The heatmap uses a colour scheme to indicate the strength and direction of correlations, with red representing a strong positive correlation, blue a strong negative correlation, and white no correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(df_features.corr(), annot=False, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8babf5",
   "metadata": {},
   "source": [
    "Here are some interpretations and how this information can be used:\n",
    "\n",
    "- **Feature selection for machine learning models**: High positive or negative correlations (indicated by red or blue colours, respectively) suggest that two features have a linear relationship. When features are highly correlated with one another (multicollinearity), it can be redundant to use all of them in a machine learning model. This information can be used to select a subset of features that provides the most information, which can improve the model's performance and reduce overfitting.\n",
    "\n",
    "- **Understanding data structure and relationships**: Correlations can reveal the underlying structure of the data. For example, a strong positive correlation between 'mean radius' and 'mean perimeter' might suggest that as the radius of a cell nucleus increases, so does its perimeter. This understanding can be vital in domains like biology or medicine, where understanding the relationships between different measurements can provide insights into the nature of cells or tumours being studied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50daaf1",
   "metadata": {},
   "source": [
    "4. **Balance of target variable:**\n",
    "\n",
    "   Lastly, we plot the distribution of the target variable using a `countplot`, showing the count of each class in the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd987f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance of target variable\n",
    "sns.countplot(x='target', data=df_target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b9ca52",
   "metadata": {},
   "source": [
    "The image is the output, where '0' and '1' represent malignant and benign conditions, respectively. The `countplot` indicates whether the dataset is balanced between these classes or if there is a class imbalance.\n",
    "\n",
    "In our case, class '1' has a higher count than class '0', indicating that there are more instances of class '1' in the dataset. This could be an indication of an **imbalance** in class distribution or an implication that **accuracy** might not be a suitable performance metric as the model could simply predict the majority class for all instances and still achieve a deceptively high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10363b6",
   "metadata": {},
   "source": [
    "### Exercise 2: Data preparation and model training\n",
    "\n",
    "Using insights gained from the EDA of the Breast Cancer dataset, we prepare the data for logistic regression by following these steps:\n",
    "\n",
    "1. **Split the data**: Given the observed class imbalance in the target variable, it's essential to maintain this distribution across training and testing datasets. We therefore use stratified splitting to ensure that both datasets represent the initial class proportions accurately.\n",
    "   \n",
    "2. **Normalise the features**: The EDA showed that the 'mean radius' is right-skewed and the 'mean texture' is closer to normally distributed. Normalisation will adjust the features to have a mean of zero and a standard deviation of one, which will help the logistic regression model to converge more efficiently since it assumes features follow a normal distribution.\n",
    "\n",
    "   **NOTE: Correlation consideration**: Correlation heatmaps reveal how features are related. If certain features are highly correlated, they may carry redundant information. To mitigate multicollinearity, we might consider feature selection or dimensionality reduction techniques. However, for the purpose of this exercise, we'll proceed with all features, but keep in mind that in practice, we may want to address this.\n",
    "\n",
    "3. **Train the model**: Fit a logistic regression model to the normalised training data. Since logistic regression can be sensitive to feature scaling, this step is crucial to ensure the model performs optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09386d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Step 1: Split the data into training and test sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, \n",
    "    data.target, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=data.target  # This ensures training and test sets have similar class proportions\n",
    ")\n",
    "\n",
    "# Step 2: Normalise the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train a logistic regression model on the training data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff325262",
   "metadata": {},
   "source": [
    "The steps and corresponding code reflect the importance of applying EDA findings to the data preparation and modelling process. This ensures that the model training is as informed and effective as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c397107a",
   "metadata": {},
   "source": [
    "### Exercise 3: Model evaluation and interpretation\n",
    "\n",
    "In this exercise, we evaluate the performance of the logistic regression model built on the Breast Cancer dataset and interpret the outcomes. By leveraging the insights from our EDA and the data preparation steps, we aim to understand not just the performance metrics but also what they tell us about the model's predictive abilities in the context of medical diagnosis. \n",
    "\n",
    "1. **Model accuracy assessment:** First, we assess the model's accuracy on the test set, which will give us a baseline understanding of its performance. Given the class imbalance noted during the EDA, we acknowledge that accuracy alone may not be the best measure of performance. Nevertheless, it provides a quick indication of overall effectiveness.\n",
    "\n",
    "2. **Advanced performance metrics:** A confusion matrix and classification report will provide a more nuanced view of the model's performance, including metrics like **precision**, **recall**, and **F1 score**. These are crucial in the medical field where the cost of false negatives could be significantly higher than false positives. \n",
    "\n",
    "3. **Coefficient analysis:** Finally, we interpret the coefficients of the model to identify which features most strongly predict the presence of malignancy in cell nuclei. This links back to our EDA findings, where we observed certain feature distributions and correlations. Understanding which features are most influential can also inform medical professionals regarding indicators of cancer severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475db79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "accuracy = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "print(f\"Model accuracy: {accuracy}\")\n",
    "\n",
    "# Generate confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(y_test, model.predict(X_test_scaled))\n",
    "class_report = classification_report(y_test, model.predict(X_test_scaled))\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "# Interpret model coefficients\n",
    "coefficients = pd.DataFrame(model.coef_[0], index=data.feature_names, columns=['Coefficient'])\n",
    "top_features = coefficients.abs().sort_values('Coefficient', ascending=False).head(3)\n",
    "print(\"Top 3 features contributing to malignancy prediction:\")\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03d36d1",
   "metadata": {},
   "source": [
    "The *model's accuracy* is 98.25%, indicating that it **correctly predicts** the outcome for a high proportion of cases in the test set. \n",
    "\n",
    "The *confusion matrix* shows that it made **only one false positive** (predicting malignancy when it is not present) and **one false negative** (failing to predict malignancy when it is present), which are both very low. \n",
    "\n",
    "The *classification report* reinforces this, as the **precision**, **recall**, and **F1 score** for both classes are all high, demonstrating that the model is equally **good at predicting both classes**.\n",
    "\n",
    "The *top three features* that contribute to predictions of malignancy are '`**worst texture**`', '`**radius error**`', and '`**worst concave points**`'. These features are the **most influential** in the logistic regression model, suggesting that they are key indicators of malignancy in breast cancer diagnosis. The high coefficients for these features mean that small changes in their values have a significant impact on the model's prediction.\n",
    "\n",
    "\n",
    "**In conclusion**: By following these steps, we can ensure that the model evaluation is comprehensive and the results are interpreted with clinical relevance in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc21e6",
   "metadata": {},
   "source": [
    "### Challenge exercise: Model improvement\n",
    "\n",
    "It's now time to leverage our findings to improve our logistic regression model. Two potential strategies will be employed: \n",
    "\n",
    "1. **Feature selection:** Reflecting on the EDA, we consider the features that showed significant correlations and distinct distributions. We will identify a subset of features that are less correlated with each other but highly correlated with the target. This approach can reduce overfitting and improve model interpretability.\n",
    "\n",
    "2. **Hyperparameter tuning:** Adjust the hyperparameters of the logistic regression model to optimise performance. The parameter '`C`', which controls the strength of regularisation, can be particularly influential. We'll use cross-validation to find the best value for '`C`' that balances the bias-variance trade-off.\n",
    "\n",
    "#### Code for model improvement:\n",
    "\n",
    "By following these steps, you will be able to not only improve your model's performance but also test the effect of each enhancement technique independently. This structured approach provides clarity on the impact of each method on model improvement and yields insights into which adjustments are most effective for this particular dataset and problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6434f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Feature Selection using Recursive Feature Elimination (RFE)\n",
    "selector = RFE(estimator=LogisticRegression(), n_features_to_select=10, step=1)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# Hyperparameter Tuning using GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluating the best model from GridSearchCV\n",
    "best_model = grid_search.best_estimator_\n",
    "best_accuracy = accuracy_score(y_test, best_model.predict(X_test_selected))\n",
    "best_conf_matrix = confusion_matrix(y_test, best_model.predict(X_test_selected))\n",
    "best_class_report = classification_report(y_test, best_model.predict(X_test_selected))\n",
    "\n",
    "# Output the improved results\n",
    "print(f\"Improved Model Accuracy: {best_accuracy}\")\n",
    "print(f\"Improved Confusion Matrix:\\n{best_conf_matrix}\\n\")\n",
    "print(f\"Improved Classification Report:\\n{best_class_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467b74f2",
   "metadata": {},
   "source": [
    "After model improvement, the accuracy slightly decreased to 97.37%. The new confusion matrix showed an additional false positive compared to the previous model, now making a total of two false positives but maintaining only one false negative. The precision remained high, but there was a slight drop in recall for class '0', resulting in a lower F1 score for that class, which combines precision and recall into a single measure.\n",
    "\n",
    "This slight decline in accuracy and recall for class '0' after feature selection and hyperparameter tuning suggests that while the model remains very accurate, the reduction of features and adjustment of hyperparameters may have led to it being slightly less sensitive to the minority class (class '0'). Simply put, the original model was already performing at a high level, leaving little room for improvement. However, the model still performs well overall with high precision and F1 scores, which are critical in a clinical setting.\n",
    "\n",
    "It is also important to note that in different contexts, even a small improvement, or maintaining high performance while reducing the model complexity (fewer features), can be valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd094b-0fee-46f1-a4b8-73766813c42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
